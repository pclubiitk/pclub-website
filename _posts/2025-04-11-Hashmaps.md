---
layout: post
title: "Deep Dive into HashMaps and Multithreading"
date: 2025-04-11 
author: Arnav Gupta
category: events
tags:
- Data Structures
- Algorithms
- Libraries
categories:
- events
image:
  url: /images/hashmap-banner.png
---
# Deep Dive into HashMaps and Multithreading

HashMaps are a cornerstone of efficient key-value storage and retrieval in computer science. They offer quick access to data, making them invaluable for various applications, including caching and database indexing.

In this blog post, we will dive into the inner workings of HashMaps, examining how they achieve such impressive performance. We'll explore their hashing function, collision resolution strategies, and underlying data structures. But the journey doesn't end there. As we delve deeper, we'll encounter the multithreaded variants of HashMaps, which are designed to handle concurrent operations with efficiency and consistency.

## 1) Basic architecture

The main idea of HashMaps is to partition data evenly using randomisation.

To do this, data is distributed between buckets,which are containers to store these key value pairs.

To determine the corresponding bucket of a pair, we use Java’s in-built function, HashCode. This function converts the data, whatever its type, into a unique number, also known as its hash. The value’s assigned bucket is the modulus of the hash of the key and the number of buckets.

**(Bucket number) \= (hash) % (number of buckets)**

So what do we do if 2 different keys collide on the same bucket? There are 2 ways to design hashmaps to handle this: open and closed hashing.

### 1.1) Open Hashing(separate chaining)

Contrary to closed hashing, buckets in open hashing are numbered placeholders that contain a number of these key-value pairs in the form of linked lists or trees.

Note: As of Java 8, when buckets are overpopulated(the default threshold is crossed if the number of elements is greater than 8), they are converted from linked lists to [red black trees](https://medium.com/basecs/painting-nodes-black-with-red-black-trees-60eacb2be9a5)(trees that arrange keys so that elements can be found in O(log n)).

![image](/images/hashmap-logic.png)
It is evident from the diagram that everytime we want to read, modify or add a value, we have to access its corresponding bucket and do the required operation.

### 1.2) Closed Hashing(open addressing)

The open addressing method allows only one value to be stored in a bucket. In case of a collision, the input to the hash is slightly modified using a probing method(read [this](https://www.geeksforgeeks.org/open-addressing-collision-handling-technique-in-hashing/) if interested).

For the purposes of this blog, we shall focus primarily on hashmaps that use open hashing.

## 2) Parameters to optimize hashmap’s performance

Clearly, there are a lot of factors in the implementation that can affect performance like:

- **initialCapacity**: initial number of buckets(impacts memory usage)
- **loadFactor**: it is the ratio of the number of elements in the map to the number of buckets. This parameter sets the limit for the load on the map instance.

In order to optimize the function according to the system’s constraints (correlated to initialCapacity) and the operation’s needs (correlated to loadFactor), hashmaps can be initialized as follows:  
```java
HashMap<key_type,value_type>(int initialCapacity, float loadFactor);
```

Initially, the number of buckets in the map is set to the initialCapacity variable. As the number of key-value pairs increases, it is more likely that we have to use a bucket with a higher population. The primary metric to estimate its performance in the average scenario is **load factor**: the ratio of the number of elements in the map to the number of buckets.

## 3) Complexity analysis  
The space complexity of this structure is O(N) where N is the number of pairs.  
If the hash is good(which is to say that it gives random values for different keys), we can assume that all data is evenly spread out between buckets. So, its time complexity for any operation would be **O(N/B)**.

### So do operations on HashMaps take linear time? 
**No.** 

Notice that N is divided by the number of buckets. This value scales with N.  
Every time the load factor of the hashmap instance crosses the limit that we set on it, the hashmap goes through a process called rehashing, wherein it doubles the number of buckets and reallocates all the key value pairs to their buckets and leaves out all deleted keys to prevent a memory leak.

So, the time complexity is actually **O(loadFactor)**.

Note: This only happens only if the hash function is good enough to distribute the keys equally or almost equally. If the hash is bad, it is possible that all keys get added to the same bucket. As a result, the time complexity for a single search or add operation can go up to O(N).

So, a high load factor causes a high time complexity but decreases space required while a low load factor causes the hashmap to take up more space but has a lesser lookup cost. In general a load factor of .75 balances both these aspects pretty well.

Though rehashing may seem expensive, notice that it happens only when the number of elements doubles after the previous rehash. So, the total complexity of all rehash calls will look something like **O(N+N/2+N/4+N/8…)** which is **\~O(N)** in the overall use of the structure.

This is actually pretty good as even the add function which is constant in time takes O(N) time to add all N pairs.

## 4) Basics of multithreading and CAS 
As systems scale up, they generally use multicore processors to address queries. Before discussing how HashMaps handle multiple requests at the same time, first let’s go over the basics of multithreading and how it works in Java.

Until the 2000s, CPU performance was boosted by increasing core speeds, but this approach stalled as cores began overheating with energy consumption above 100W, leaving clock speeds stagnant. A new idea, which would prove to be much more efficient and give birth to the idea of multithreading, was to use multiple cores in a single silicon chip. With the necessary software support, each core could run a series of processes, also known as : a **thread**.

![image](/images/hashmap-pictureOfCPU.png)
This is where multithreading took off. With multithreading, a computer could run multiple processes in parallel. This was very useful in multiple fields like web servers, cloud computing and faster information modification.

There are 2 types of multithreading: **pessimistic and optimistic**. Pessimistic multithreading uses locking mechanisms(like the synchronized keyword in Java) on the data object to ensure that there is no thread that interferes with the current operation. While this approach is safe, it is only partially better than having a single thread do all the operations and has poorer performance than its optimistic counterpart.

Optimistic multithreading runs all threads simultaneously. But if we allow all these threads to make changes as and when they are evaluated, we observe inconsistency in the obtained result. Take the case where a variable a is 0. If two separate threads that increment it are called simultaneously, both threads take the initial value of a as 0 and set it to 1 individually. But a has been incremented twice and should have a value of 2(which would be the case if they were executed one after the other). This issue where the order in which threads execute affects the output is called the **race condition**.  
![image](/images/hashmap-parallelThreads.png)

To solve this issue, Java uses the **CAS(compare and swap)** system. Before committing the last atomic(fundamental) operation, which happens to be assigning the variable to its new value, the algorithm checks if the existing value is equal to the value stored in the register before the value was evaluated. If it is the same, the new value is set, otherwise the operation is executed again(as it is evident another thread has already made some modifications to it) using the new values of the variable for evaluation.  
![image](/images/hashmap-casLogic.png)
It is evident that when there are multiple threads that modify different locations in memory(thus reducing collisions), optimistic multithreading would be significantly more efficient than its pessimistic counterpart. Thus, modifying operations to hashmaps could be significantly sped up if they are multithreaded optimistically. In the remainder of the blog, we shall see different structures implement these kinds of multithreading.

## 5) Pessimistic model \- HashTables and Synchronized HashMaps

Similar in structure to hashmaps, HashTables deal with multithreading using pessimistic multithreading. Sun.misc.Unsafe.park, similar to calling thread.wait(), defined in the Unsafe class of JDK(Java Development Kit) are used to lock data structures when a thread operates on it by parking all other threads. Sun.misc.Unsafe.unpark used by JVM to make the thread runnable again. Note that read functions neither cause locks nor are affected by locks.

#### Why not just use thread.wait() on all other threads?

The park function is preferred over thread.wait in the implementation because it calls native OS code that takes advantage of some architecture specifics to get the best performance.

While hash tables make multithreading possible, they are very slow. Let us see this in action. (check this [blog](https://medium.com/@RamLakshmanan/java-hashtable-hashmap-concurrenthashmap-performance-impact-2e18b813201d) to see the implementation)  
Hashtables in the above example took nearly an order of magnitude more than hashmaps and concurrent hashmaps to process random modification operations.

Note that hash tables and synchronized hashmaps differ only in topics related to iterators and null key values. Their method of remaining thread protected is nearly the same. So, the following also applies to synchronized hashmaps.

## 6) Semi Optimistic model \- Concurrent hashmaps  
Concurrent hashmaps are semi optimistic in the sense that the structure is partitioned into buckets each of which employ pessimistic multithreading. So, the structure as a whole can be simultaneously modified by multiple threads as long as they modify distinct substructures.

In Java 7, a set number of segments(which are similar in function to HashTables) are declared on initialisation, representing buckets. These segments are completely locked when they are modified. Hash values for finding the segment and bin within the segment are also calculated differently to allow distribution of keys, although a uniform distribution is not guaranteed. Modify operations are halted if the segment is going through resizing and an indicator on the segment signals whether the segment has been rehashed into the new hashMap(in which case modifications and read operations are made into the newer hashMap).

![image](/images/hashmap-concurrent.png)
In Java 8, each bucket is either a linked list or a red black tree(similar to HashMaps in java 8). Modifications to any of these cause the bucket to be locked for other modify operations. While read operations on linked lists are not affected by locks, read operations on locked trees are halted as modifications can cause rotations that would interfere with read operations. During resizing, multiple threads are set up and assigned blocks of indices to transfer the key value pairs of the corresponding buckets in parallel.This also allows other modification operations to execute in parallel on the resized map, without prolong locking of structures.  
![image](/images/hashmap-concurrent2.png)
## 7) Optimistic model \- Non Blocking HashMaps

Non Blocking Hashmaps follow a completely optimistic approach to handle multithreading. This is made possible using CAS. Modification operations can operate on the same bin in parallel as long as they use CAS just before the last atomic operation. This is very efficient as now even operations to the same bin are not completely halted.  
Though it is not included in the standard JDK, it can be imported from [here](https://github.com/boundary/high-scale-lib/blob/master/src/main/java/org/cliffc/high_scale_lib/NonBlockingHashMap.java)  
For a clearer and deeper understanding on this structure, refer to [this](https://www.youtube.com/watch?v=HJ-719EGIts&t=2184s) video by Cliff Click. The following is a brief summary of a specific aspect of what is shown in the video

#### But how do these operations work during resizing?
To explain resizing, we must first go over some update states.  
![image](/images/hashmap-nbhm-new-table.png)
**K** \- Key  
**V** \- Value associated to key  
**Null** \- uninitialised variable  
**T** \- Tombstone(when a key is deleted)  
**X** \- indication to use the new table

Firstly, The rehash operation does not copy keys with a tombstone value(implying they were deleted) to prevent a key leak(similar to memory leak but with keys).

While reading, the older table is first accessed to get the key value pair. If the key has not been rehashed yet, its state will be V, otherwise its value is set to X and the new table will have to now be accessed for the value.

Modification operations use the new table.  
#### But how does the resize operation ensure that it does not override any modifications in the new table? 
![image](/images/hashmap-nbhm-copying.png)  
Here $V’$(known as primed value) stands for the value associated with K that was copied from the old table to the new table.  
Primed values essentially serve as an indicator that the copy has not finished. Internally it is ensured that the system can tell primed values and actual values apart, i.e $V_1’$ and $V_1$ are distinct states by simply using an indicator for the same(a particular bit or a particular object).  
Initially the value in the new table is set to $V_x’$.  
First, we check whether the value of the key in the new table is primed or not. If the state is not primed, it is evident that new updates have been made and hence the operation is canceled. In the other case, we read the value of $V_1$ and copy its primed state($V_1’$) into the new map. Now, we check if the $V_1$ in the older table has been changed due to any late-coming update operations.If it has, we retry the thread otherwise we set it to X to direct any operations on the key to the new map. Consequently, we CAS the value at K in the new map and set it to $V_1$ if it is still $V_1’$ and redo the process if it is not.

This ensures that read and modify operations to the data is consistent and accurate without the need for locks, making it truly optimistic.

Author: Arnav Gupta
